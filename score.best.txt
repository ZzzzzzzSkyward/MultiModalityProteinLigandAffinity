target
pearson0.8
tau0.8
rho0.8
protein autoencoder 64 batchnorm1 dropout=0.5 epoch=50
必须加batchnorm，否则只有0.6,0.6,0.45,0.45左右
(pytorch) zzs_dlib@dlib-SYS-4028GR-TR:~/mult$ python main.py --test --pretrained  --detailed --name="test_protein_seq_64"
04.16.16:42:37[INFO]pearson=[0.7091485660891095,9.111622504404653e-24],rmse=1.4951659440994263,tau=[0.5200485707655955,9.579051196374436e-21],rho=SpearmanrResult(correlation=0.6884351583115776, pvalue=5.67942089540966e-22)
04.16.16:42:42[INFO]pearson=[0.607719522411857,9.057503485817135e-31],rmse=1.575845718383789,tau=[0.4007924124335955,2.3128837264305614e-24],rho=SpearmanrResult(correlation=0.5647545475470157, pvalue=6.466988405329585e-26)
04.16.16:42:44[INFO]pearson=[0.4734152476781245,3.23344715238527e-11],rmse=1.5696355104446411,tau=[0.3018404020181371,2.8592551139464587e-09],rho=SpearmanrResult(correlation=0.42623590457672195, pvalue=3.6758368028162024e-09)
04.16.16:42:47[INFO]pearson=[0.4715047205661128,8.591388552352102e-11],rmse=1.8150978088378906,tau=[0.3157787296972342,1.0289632530977706e-09],rho=SpearmanrResult(correlation=0.46401263849894236, pvalue=1.8535058892838281e-10)
plain gru batchnorm1 dropout=0.3 epoch=60
04.17.10:15:55[INFO]pearson=[0.6514896465699946,4.067403242510191e-19],rmse=1.5599662065505981,tau=[0.45384192108950216,3.595546938172507e-16],rho=SpearmanrResult(correlation=0.6277916617484417, pvalue=1.7424803794203404e-17)
04.17.10:15:59[INFO]pearson=[0.6126678925109719,2.2374349012420255e-31],rmse=1.561119556427002,tau=[0.4113720416917266,1.4064760655795321e-25],rho=SpearmanrResult(correlation=0.5828076676905946, pvalue=7.222712489078899e-28)
04.17.10:16:00[INFO]pearson=[0.45282688342701716,2.78916653672448e-10],rmse=1.631085991859436,tau=[0.28831545509444634,1.4003721185927535e-08],rho=SpearmanrResult(correlation=0.40423002833844485, pvalue=2.639650567255779e-08)
04.17.10:16:02[INFO]pearson=[0.49470106670413155,7.0366595371873566e-12],rmse=1.7535725831985474,tau=[0.3350182284602445,9.368657626879124e-11],rho=SpearmanrResult(correlation=0.4834342050928583, pvalue=2.428742331253867e-11)
plain gru batchnorm1 dropout=0.3 varied batch size, varied epoch
04.18.19:31:56[INFO]pearson=0.6794,rmse=2.06,tau=0.4700,rho=0.6518
04.18.19:32:00[INFO]pearson=0.3407,rmse=2.75,tau=0.3682,rho=0.5172
04.18.19:32:03[INFO]pearson=0.3944,rmse=1.77,tau=0.2701,rho=0.3826
04.18.19:32:05[INFO]pearson=0.5640,rmse=1.82,tau=0.3871,rho=0.5547
2048长度，间隔0后学习难度变大，预训练epoch=160，epoch=10
04.18.21:18:32[INFO]pearson=55.29,rmse=1.76,tau=33.44,rho=46.28
04.18.21:18:47[INFO]pearson=36.05,rmse=1.84,tau=18.68,rho=27.35
04.18.21:18:54[INFO]pearson=41.95,rmse=1.61,tau=21.18,rho=30.35
04.18.21:19:04[INFO]pearson=41.38,rmse=1.90,tau=25.48,rho=36.62
epoch=20
04.18.21:26:23[INFO]pearson=64.19,rmse=1.61,tau=43.38,rho=61.09
04.18.21:26:32[INFO]pearson=50.20,rmse=1.70,tau=32.61,rho=47.07
04.18.21:26:38[INFO]pearson=48.60,rmse=1.56,tau=30.81,rho=43.63
04.18.21:26:47[INFO]pearson=47.91,rmse=1.81,tau=30.82,rho=45.23
逆转序列，尝试解决遗忘问题。从之前的预训练模型继续，但是貌似效果更加不好
epoch=17
04.18.21:55:12[INFO]pearson=57.50,rmse=1.70,tau=37.63,rho=52.62
04.18.21:55:23[INFO]pearson=46.36,rmse=1.74,tau=29.77,rho=42.75
04.18.21:55:30[INFO]pearson=47.04,rmse=1.58,tau=28.00,rho=39.14
04.18.21:55:36[INFO]pearson=49.16,rmse=1.80,tau=32.04,rho=46.32
修改batch size=64重新跑一次，发现下降速度很慢，lr=0.1和1e-6好像都一样
